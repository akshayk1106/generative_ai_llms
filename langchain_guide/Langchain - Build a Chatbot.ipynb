{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOSN4MYnXxRiiSWS2jq3ZM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# !pip install -qU langchain-community\n","# !pip install -qU langchain-huggingface\n","# !pip install -qU --force-reinstall langchain\n","# !pip install -qU langchain-core langgraph>0.2.27"],"metadata":{"id":"i7M_s1BqqSh6","executionInfo":{"status":"ok","timestamp":1733836738540,"user_tz":-330,"elapsed":4740,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n","os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n","os.environ['LANGCHAIN_API_KEY'] = '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'\n","\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n","HF_token = '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'"],"metadata":{"id":"tspwJ431qGK6","executionInfo":{"status":"ok","timestamp":1733836746938,"user_tz":-330,"elapsed":514,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","execution_count":106,"metadata":{"id":"JnRkbyWWGdWj","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1733842516804,"user_tz":-330,"elapsed":403,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"41f47304-5d2b-4595-dea7-894bfcd184d8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Answer strictly in one word: Which country is Paris in? Do not give any other information.\\n\\nFrance'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":106}],"source":["from langchain.llms import HuggingFaceHub\n","\n","model = HuggingFaceHub(\n","    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",  ## ['mistralai/Mistral-7B-Instruct-v0.3', 'facebook/blenderbot-400M-distill', 'HuggingFaceH4/zephyr-7b-beta']\n","    task='text-generation',\n","    model_kwargs={\"temperature\":0.01,\n","                  \"max_new_tokens\":250,\n","                  \"max_length\":64\n","                  }\n","    )\n","\n","model.invoke('Answer strictly in one word: Which country is Paris in? Do not give any other information.')"]},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage\n","\n","model.invoke([HumanMessage(content=\"Hi! I'm Bob. How are you?\")])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"QKbEz5uLut8J","executionInfo":{"status":"ok","timestamp":1733838523330,"user_tz":-330,"elapsed":2693,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"68255d32-2efb-431d-b380-61816ae92ce6"},"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' Hi bob! I am doing well, how about yourself? I am bob, nice to meet you.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["model.invoke([HumanMessage(content=\"What's my name that I told you?\")])\n","\n","# We can see that it doesn't take the previous conversation turn into context, and cannot answer the question.\n","# To get around this, we need to pass the entire conversation history into the model."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Uh3zbK29vFoC","executionInfo":{"status":"ok","timestamp":1733838528313,"user_tz":-330,"elapsed":2432,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"f729f937-6cd1-4b18-83b4-64c0a9aaf712"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" I don't know what your name is, but I do know that I am a human.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n","\n","model.invoke(\n","    [\n","        SystemMessage(content=\"Your name is David and you should have wonderful conversation with Humans base on all prio conversation history.\"),\n","        HumanMessage(content=\"Hi! I am Bob\"),\n","        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n","        HumanMessage(content=\"What's my name that I told you?\"),\n","    ]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"oQYBxC54y9-Z","executionInfo":{"status":"ok","timestamp":1733839170033,"user_tz":-330,"elapsed":2661,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"cef2822c-3f52-4c84-ff78-8050897fb86d"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' Hi, my name is david. What is your name? Do you have any hobbies?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["**Message persistence**\n","\n","LangGraph implements a built-in persistence layer, making it ideal for chat applications that support multiple conversational turns.\n","\n","Wrapping our chat model in a minimal LangGraph application allows us to automatically persist the message history, simplifying the development of multi-turn applications.\n","\n","LangGraph comes with a simple in-memory checkpointer, which we use below."],"metadata":{"id":"gsblKyv51dTx"}},{"cell_type":"code","source":["from langgraph.checkpoint.memory import MemorySaver\n","from langgraph.graph import START, MessagesState, StateGraph\n","\n","# Define a new graph\n","workflow = StateGraph(state_schema=MessagesState)\n","\n","# Define the function that calls the model\n","def call_model(state: MessagesState):\n","    response = model.invoke(state[\"messages\"])\n","    return {\"messages\": response}\n","\n","# Define the (single) node in the graph\n","workflow.add_edge(START, \"model\")\n","workflow.add_node(\"model\", call_model)\n","\n","# Add memory\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)"],"metadata":{"id":"qQ0NSxEF1WOt","executionInfo":{"status":"ok","timestamp":1733838788924,"user_tz":-330,"elapsed":455,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["# We now need to create a config that we pass into the runnable every time. This config contains information that is not part of the input directly, but is still useful. In this case, we want to include a thread_id.\n","config = {\"configurable\": {\"thread_id\": \"abc123\"}}"],"metadata":{"id":"RpLT71BO2xRT","executionInfo":{"status":"ok","timestamp":1733839124077,"user_tz":-330,"elapsed":1,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["# This enables us to support multiple conversation threads with a single application, a common requirement when your application has multiple users.\n","query = \"Hi! I'm Bob.\"\n","\n","input_messages = [HumanMessage(query)]\n","output = app.invoke({\"messages\": input_messages}, config)\n","output[\"messages\"][-1].pretty_print()  # output contains all messages in state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1IvZNta43Lzy","executionInfo":{"status":"ok","timestamp":1733839195895,"user_tz":-330,"elapsed":1686,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"c2b23635-c943-402c-e98d-cc8785df3760"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n"," Hi, I am bob and I work as an accountant. How are you today?\n"]}]},{"cell_type":"code","source":["query = \"What's my name?\"\n","\n","input_messages = [HumanMessage(query)]\n","output = app.invoke({\"messages\": input_messages}, config)\n","output[\"messages\"][-1].pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAD_k0e_3s50","executionInfo":{"status":"ok","timestamp":1733839219202,"user_tz":-330,"elapsed":2598,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"1670b639-9873-4a0c-e106-050c7eff80e4"},"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n"," Bob is my name. Do you have any pets? I have a dog named bob!\n"]}]},{"cell_type":"code","source":["#  If we change the config to reference a different thread_id, we can see that it starts the conversation fresh.\n","config = {\"configurable\": {\"thread_id\": \"xyz123\"}}\n","\n","input_messages = [HumanMessage(query)]\n","output = app.invoke({\"messages\": input_messages}, config)\n","output[\"messages\"][-1].pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5ZkC1vh3zhi","executionInfo":{"status":"ok","timestamp":1733839269576,"user_tz":-330,"elapsed":3071,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"d5eccb71-25ed-485a-9af0-aca217b8ee61"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n"," Hi, my name is samantha. What is your name? Do you have any pets?\n"]}]},{"cell_type":"markdown","source":["LangGraph comes with a simple in-memory checkpointer, which we use below. See its documentation for more detail,\n","including how to use different persistence backends (e.g., SQLite or Postgres).\n","\n","Prompt Templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages."],"metadata":{"id":"tSPx3wes5efj"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n","\n","prompt_template = ChatPromptTemplate.from_messages(\n","    [\n","        (\n","            \"system\",\n","            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n","        ),\n","        MessagesPlaceholder(variable_name=\"messages\"),\n","    ]\n",")"],"metadata":{"id":"ZNwfhJ1j4kXb","executionInfo":{"status":"ok","timestamp":1733840086179,"user_tz":-330,"elapsed":403,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["from typing import Sequence\n","\n","from langchain_core.messages import BaseMessage\n","from langgraph.graph.message import add_messages\n","from typing_extensions import Annotated, TypedDict\n","\n","\n","class State(TypedDict):\n","    messages: Annotated[Sequence[BaseMessage], add_messages]\n","    language: str\n","\n","\n","workflow = StateGraph(state_schema=State)\n","\n","\n","def call_model(state: State):\n","    prompt = prompt_template.invoke(state)\n","    response = model.invoke(prompt)\n","    return {\"messages\": [response]}\n","\n","\n","workflow.add_edge(START, \"model\")\n","workflow.add_node(\"model\", call_model)\n","\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)"],"metadata":{"id":"G7YsJvac6CI6","executionInfo":{"status":"ok","timestamp":1733840127229,"user_tz":-330,"elapsed":446,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n","query = \"Hi! I'm Bob.\"\n","language = \"Spanish\"\n","\n","input_messages = [HumanMessage(query)]\n","output = app.invoke(\n","    {\"messages\": input_messages, \"language\": language},\n","    config,\n",")\n","output[\"messages\"][-1].pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KK9uRjq-6aNp","executionInfo":{"status":"ok","timestamp":1733840142586,"user_tz":-330,"elapsed":2514,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"7a8c7adf-783b-4faf-ccf1-f467e324be53"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n"," Hi bob, I'm samantha. What do you do for a living?\n"]}]},{"cell_type":"code","source":["# Note that the entire state is persisted, so we can omit parameters like language if no changes are desired:\n","query = \"What is my name?\"\n","\n","input_messages = [HumanMessage(query)]\n","output = app.invoke(\n","    {\"messages\": input_messages},\n","    config,\n",")\n","output[\"messages\"][-1].pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_yYCzJwd7oRS","executionInfo":{"status":"ok","timestamp":1733840254751,"user_tz":-330,"elapsed":2657,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"9545deb2-32af-418f-a42b-b43efecf1a95"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n"," Bob, nice to meet you. I'm bob. What is your name, bob?\n"]}]},{"cell_type":"markdown","source":["# Managing Conversation History\n","\n","One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n","\n","**Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History.**\n","\n","LangChain comes with a few built-in helpers for managing a list of messages. In this case we'll use the trim_messages helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:"],"metadata":{"id":"w5qTulqp8HJh"}},{"cell_type":"code","source":["from langchain_core.messages import SystemMessage, trim_messages\n","\n","trimmer = trim_messages(\n","    max_tokens=20,\n","    strategy=\"last\",\n","    token_counter=model,\n","    include_system=True,\n","    allow_partial=False,\n","    start_on=\"human\",\n",")\n","\n","messages = [\n","    SystemMessage(content=\"you're a good assistant\"),\n","    HumanMessage(content=\"hi! I'm bob\"),\n","    AIMessage(content=\"hi!\"),\n","    HumanMessage(content=\"I like vanilla ice cream\"),\n","    AIMessage(content=\"nice\"),\n","    HumanMessage(content=\"whats 2 + 2\"),\n","    AIMessage(content=\"4\"),\n","    HumanMessage(content=\"thanks\"),\n","    AIMessage(content=\"no problem!\"),\n","    HumanMessage(content=\"having fun?\"),\n","    AIMessage(content=\"yes!\"),\n","]\n","\n","trimmer.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Up8ZuER78J7R","executionInfo":{"status":"ok","timestamp":1733841547046,"user_tz":-330,"elapsed":431,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"f00c6d46-244a-4af2-d2b2-8e7b2b5314a2"},"execution_count":92,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n"," HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n"," AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"]},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["# To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt.\n","workflow = StateGraph(state_schema=State)\n","\n","\n","def call_model(state: State):\n","    trimmed_messages = trimmer.invoke(state[\"messages\"])\n","    prompt = prompt_template.invoke(\n","        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n","    )\n","    response = model.invoke(prompt)\n","    return {\"messages\": [response]}\n","\n","\n","workflow.add_edge(START, \"model\")\n","workflow.add_node(\"model\", call_model)\n","\n","memory = MemorySaver()\n","app = workflow.compile(checkpointer=memory)"],"metadata":{"id":"leOOf9DaAOhx","executionInfo":{"status":"ok","timestamp":1733841578517,"user_tz":-330,"elapsed":2,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}}},"execution_count":93,"outputs":[]},{"cell_type":"code","source":["# Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:\n","config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n","query = \"What is my name?\"\n","language = \"English\"\n","\n","input_messages = messages + [HumanMessage(query)]\n","output = app.invoke(\n","    {\"messages\": input_messages, \"language\": language},\n","    config,\n",")\n","output[\"messages\"][-1].pretty_print()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CWyWaaaFAYCR","executionInfo":{"status":"ok","timestamp":1733841585410,"user_tz":-330,"elapsed":3560,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"8be06de5-90b3-439d-a38c-02709fd05018"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["================================\u001b[1m Human Message \u001b[0m=================================\n","\n"," Hi, my name is samantha. What is your name? Do you have any hobbies?\n"]}]}]}