{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPRAFXyu8x8fWJyyDBGe48f"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# <mark> Build a Retrieval Augmented Generation (RAG) App\n","\n","    Familiarize yourself with LangChain's open-source components by building simple applications:\n","    https://python.langchain.com/docs/tutorials/\n","    https://python.langchain.com/docs/how_to/\n","    https://python.langchain.com/docs/concepts/\n","\n","\n","    PART 1: https://python.langchain.com/docs/tutorials/rag/\n","    PART 2: https://python.langchain.com/docs/tutorials/qa_chat_history/\n","\n","    RAG PROJECTS: https://github.com/atulkashyap404/RAG-Projects\n","\n","    Tensorflow in 30 days: https://github.com/lyhue1991/eat_tensorflow2_in_30_days\n"],"metadata":{"id":"QNDFB2Zop7ZI"}},{"cell_type":"code","source":["# !pip install --quiet langchain_community tiktoken langchainhub chromadb langchain-text-splitters\n","# !pip install --quiet langchain langchain-community langchain-core langchain-openai\n","# !pip install --upgrade httpx==0.27.2\n","# !pip install -qU langchain-chroma\n","# !pip install -qU langgraph\n","# !pip install --quiet transformers torch einops accelerate\n","# !pip install -qU huggingface-hub langchain-huggingface\n","# !pip install -qU llama-cpp-python"],"metadata":{"id":"U38vyQGZxePK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n","os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n","os.environ['LANGCHAIN_API_KEY'] = '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'\n","\n","os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n","HF_token = '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'\n","\n","os.environ['OPENAI_API_KEY'] = 'sk-'\n","\n","# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n","# if not os.environ.get(\"LANGCHAIN_API_KEY\"):\n","#     os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"],"metadata":{"id":"hCMAxN7buh23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","embeddings = HuggingFaceInferenceAPIEmbeddings(api_key = HF_token, model_name = \"BAAI/bge-base-en-v1.5\")\n","\n","text = \"What is deep learning?\"\n","query_result = embeddings.embed_query(text)\n","query_result[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CcjNhbUFK3AQ","executionInfo":{"status":"ok","timestamp":1733767539090,"user_tz":-330,"elapsed":434,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"803c581c-ec6b-4657-d4f3-077f64c6a8a6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.0003711276513058692, -0.06356814503669739, 0.0024758256040513515]"]},"metadata":{},"execution_count":89}]},{"cell_type":"code","source":["# from langchain_openai import ChatOpenAI\n","# llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n","\n","# from langchain_openai import AzureChatOpenAI\n","# llm = AzureChatOpenAI(\n","#     azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n","#     azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n","#     openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n","# )\n","\n","# # Other options:\n","# OpenAI, Anthropic, Azure, Google, AWS, Cohere, NVIDIA, FireworksAI, Groq, MistralAI, TogetherAI, Databricks,"],"metadata":{"id":"S31M_beOwYp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## # a vector store:\n","# In-memory, AstraDB, Chroma, FAISS, Milvus, MongoDB, PGVector, Pinecone, Qdrant,# from langchain.vectorstores import Chroma\n","\n","from langchain_core.vectorstores import InMemoryVectorStore\n","from langchain_chroma import Chroma\n","\n","vector_store = Chroma(embedding_function=embeddings)\n","vector_store_inmemory = InMemoryVectorStore(embeddings)"],"metadata":{"id":"0i9wfvSU0s3i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <mark> Build an app that answers questions about the website's content.\n","\n","The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n"],"metadata":{"id":"fFtLHb7S1z13"}},{"cell_type":"code","source":["# Build an app that answers questions about the website's content. The specific website we will use is the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.\n","\n","import bs4\n","from langchain import hub\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_core.documents import Document\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langgraph.graph import START, StateGraph\n","from typing_extensions import List, TypedDict"],"metadata":{"id":"GSIh0GPR1TPx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load and chunk contents of the blog\n","loader = WebBaseLoader(\n","    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n","    bs_kwargs=dict(\n","        parse_only=bs4.SoupStrainer(\n","            class_=(\"post-content\", \"post-title\", \"post-header\")\n","        )\n","    ),\n",")\n","docs = loader.load()"],"metadata":{"id":"XfYfq0UX152R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","all_splits = text_splitter.split_documents(docs)\n","\n","print(type(all_splits))\n","print(len(all_splits))\n","print(len(all_splits[0].page_content))\n","print(all_splits[0].page_content[0:400])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rt2117wU2G7T","executionInfo":{"status":"ok","timestamp":1733767565410,"user_tz":-330,"elapsed":482,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"34e8b784-dc03-4e16-feb7-b2e60480cea4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'>\n","66\n","969\n","LLM Powered Autonomous Agents\n","    \n","Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n","\n","\n","Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays an\n"]}]},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n","from langchain.embeddings import HuggingFaceEmbeddings\n","\n","embeddings = HuggingFaceInferenceAPIEmbeddings(api_key = HF_token, model_name = \"BAAI/bge-base-en-v1.5\")\n","\n","text = \"What is deep learning?\"\n","query_result = embeddings.embed_query(text)\n","query_result[:3]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dh9BniiQNjW5","executionInfo":{"status":"ok","timestamp":1733767569378,"user_tz":-330,"elapsed":2,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"a39c79cd-55bf-411e-9cbb-9d32ef5e99e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.0003711276513058692, -0.06356814503669739, 0.0024758256040513515]"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","source":["from langchain_chroma import Chroma\n","\n","vector_store = Chroma(embedding_function=embeddings)"],"metadata":{"id":"1xoIt_IjNU9h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Index chunks: this gives IDs for each chunk\n","vector_store.add_documents(documents=all_splits)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"be_sOgCp3b1c","executionInfo":{"status":"ok","timestamp":1733767602088,"user_tz":-330,"elapsed":12638,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"059a452e-286d-4190-9450-83410e5e452d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['c95a8036-1189-4537-bfcf-f7b0a74587d9',\n"," '0d65c10e-9bd8-423a-b02c-ebdee8c115ad',\n"," 'cb7147ee-9d89-478d-a4e0-67c5e32d7ac6',\n"," 'df6d79aa-fa38-4d3e-ac17-712282dd83d9',\n"," '798f0351-5ec3-45d7-bb97-1bd9f259f7b9',\n"," '2c00a6e6-3cdd-496a-ad2b-08a163786f27',\n"," '721e70bf-b1bb-4d33-9283-f1c6999baea4',\n"," '02b8986c-39b8-4483-bcff-0ed17fc1536b',\n"," '5a1846d4-89ff-4c70-9c02-828262fdb9b4',\n"," '6e489523-edae-4338-b323-29439791ab56',\n"," '4cf34550-2528-47dd-9690-086a2b1975a9',\n"," '3513b292-92e7-4be3-be76-a5432e7bd20a',\n"," '0318acbe-9a65-41d6-a988-8b5eed56cfe4',\n"," '98eff818-e872-40b1-8d48-1fab50a8a63a',\n"," '7edb10be-39a0-4529-a2d5-457d9b401354',\n"," '217bf14a-6b1e-4bba-b895-6fc078e8fb11',\n"," 'b3754ffe-a4cd-4172-916a-3567acac1c6f',\n"," '1b6e160a-0546-4a24-9ee2-65ce79bc7d9f',\n"," '34425a2c-c9c0-45c5-b016-76a3fa2f060c',\n"," 'bf656894-89db-47d9-b76e-1e155e72a1fb',\n"," '3459f7f5-928b-4b9e-a70e-bec258d8153f',\n"," 'f4348696-d767-4dbe-9ca2-6d42ca4985f0',\n"," '816732af-ce52-471b-80f4-0af437146010',\n"," '038acd71-9695-409b-960b-f457915e8747',\n"," '75824a3f-862a-4588-b6ce-aa1be821e53b',\n"," 'fdc48fb9-f39f-4465-bfd1-ae8a1ac263e9',\n"," '2a35d04a-0f62-42c0-a2eb-5a58828c3a3c',\n"," '8492b01f-e57d-4a38-a164-80912a5807f2',\n"," 'b19559b8-902b-4337-a46f-473b81d084be',\n"," '00957d39-cd5e-4473-b4a7-640d4f3eb2f4',\n"," '04d2ed35-709d-47eb-b6bd-86f32a5a5ca5',\n"," '15a68ede-90f0-448c-8eb6-b0ceb5729631',\n"," '34dfee4a-dcab-40c5-b9a3-eb8e471f1cfe',\n"," '94311ad9-8ede-433f-b070-43446594e3fb',\n"," '7f05ef1e-b297-4365-b7a9-ebc95ce21576',\n"," '96a8df14-4414-4824-bf38-3f36d2beac35',\n"," '23f18e91-cb04-4672-a45a-d7f0cb91113b',\n"," '15a6ecbd-a9f9-4b1b-a025-64ab2fecc372',\n"," 'b9da89b5-75f1-4dca-96a1-4b81217e72f4',\n"," '276c985d-6361-430b-866e-7c1f73407769',\n"," '70f8717a-01ca-436a-b70c-9a8b4091fc86',\n"," '3b65edd3-5772-46ab-9a86-bc6e3c307f43',\n"," 'a4b486db-8712-4727-bfd4-db20cfdcd0ea',\n"," '25df2beb-61ed-4427-91c8-f2ff89084caa',\n"," '1ae99f91-9c68-4e4a-99f0-d555abd1573e',\n"," '7bd09f84-c520-4d01-9083-83542344c295',\n"," 'a41261d3-913c-4cf4-a24f-dafff987a475',\n"," '22b51092-0332-4847-92a6-001f5925eeac',\n"," '4e7a6b02-459d-41b9-9c48-cf58828e279b',\n"," 'e6e36a02-955e-4a63-94e4-fdf5207db7f5',\n"," 'b005c8f3-668e-42f3-a994-5299b3d8dcf3',\n"," '40150a64-85c0-49fb-9e86-4c33a6d59b02',\n"," 'd9552100-b6aa-4112-afdb-26f747c950f9',\n"," 'cfba58ae-bcfe-4568-80a8-c2894cbbfcd9',\n"," '7f7139b3-5f4d-421d-a3f7-c9472ce4cb37',\n"," 'e4c78f7b-8676-44cf-aec4-25cbd79d205d',\n"," '5da975b7-62de-4e56-b17a-e738c65b1768',\n"," 'a3c19746-76b4-4c63-bca5-3316ce3fd749',\n"," '446f16f6-7283-476d-ab8e-8ef7aad619dc',\n"," '2b1e5dcd-5ae6-4dd8-a9a3-68a4bbe78fd0',\n"," '8ed00acc-52d3-4e81-8715-3aa3cd6f0d89',\n"," '9dbf5bc1-26c5-4146-ac61-f2d5167c09c0',\n"," 'c4ad1f43-bd4c-4ab8-a56a-d4632f1777f1',\n"," '309aa637-916b-4f35-9df3-5a6f03bd0c97',\n"," '5883a264-306a-4d03-a6db-e5d6cce4f27b',\n"," 'd715a1ab-87c3-4de5-8356-483042802783']"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["retriever = vector_store.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":3})\n","query = \"what is recurrent neural network?\"\n","docs_rel = retriever.get_relevant_documents(query)\n","print(docs_rel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NMSPK_ZANO14","executionInfo":{"status":"ok","timestamp":1733767612125,"user_tz":-330,"elapsed":438,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"28c07bcf-04a7-4533-d925-5730fa1210b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.')]\n"]}]},{"cell_type":"code","source":["prompt = f\"\"\"\n","<|system|>>\n","You are an AI Assistant that follows instructions extremely well.\n","Please be truthful and give direct answers. Please tell 'I don't know' if user query is not in context\n","</s>\n","<|user|>\n","{query}\n","</s>\n","<|assistant|>\n","\"\"\""],"metadata":{"id":"9tmdqHReN-W4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## The Large language model that we will use is Zephyr-7B model fine-tuned model from Mistral-7B.\n","from langchain.llms import HuggingFaceHub\n","from langchain.chains import RetrievalQA\n","\n","model = HuggingFaceHub(repo_id=\"HuggingFaceH4/zephyr-7b-alpha\",\n","                       model_kwargs={\"temperature\":0.5,\n","                                     \"max_new_tokens\":512,\n","                                     \"max_length\":64\n","                                    })\n","\n","qa = RetrievalQA.from_chain_type(llm=model,retriever=retriever,chain_type=\"stuff\")\n","response = qa(prompt)\n","print(response['result'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9mAbHgCN_9S","executionInfo":{"status":"ok","timestamp":1733767633280,"user_tz":-330,"elapsed":4918,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"56a450e6-147c-448a-8dfe-f8056311aad8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n","\n","Here are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\n","[\n","  {\n","    \"role\": \"system\",\n","    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\nThen you will pick one clarifying question, and wait for an answer from the user.\\n\"\n","  },\n","  {\n","    \"role\": \"user\",\n","    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\n\"\n","  },\n","  {\n","    \"role\": \"assistant\",\n","\n","Fig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\n","The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\n","In reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\n","\n","Or\n","@article{weng2023agent,\n","  title   = \"LLM-powered Autonomous Agents\",\n","  author  = \"Weng, Lilian\",\n","  journal = \"lilianweng.github.io\",\n","  year    = \"2023\",\n","  month   = \"Jun\",\n","  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n","}\n","References#\n","[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\n","[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\n","[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\n","“ arXiv preprint arXiv:2302.02676 (2023).\n","[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\n","[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\n","[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\n","[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\n","\n","Question: \n","<|system|>>\n","You are an AI Assistant that follows instructions extremely well.\n","Please be truthful and give direct answers. Please tell 'I don't know' if user query is not in context\n","</s>\n","<|user|>\n","what is recurrent neural network?\n","</s>\n","<|assistant|>\n","\n","Helpful Answer:\n","\n","Recurrent Neural Networks (RNNs) are a type of artificial neural network that can process sequences of data. They have the ability to remember previous inputs and use that information to affect the output of the network. This is achieved by using a feedback mechanism, where the output of the network at a given time step is fed back into the input at the next time step. This allows the network to maintain a \"memory\" of past inputs and process them in a sequential manner. RNNs are commonly used in natural language processing, speech recognition, and other applications where sequences of data are important.\n"]}]},{"cell_type":"code","source":["import transformers\n","import torch\n","\n","model_name='EleutherAI/gpt-neo-125m'\n","tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n","model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n","\n","pipeline = transformers.pipeline(\n","    'text-generation',\n","    model=model,\n","    tokenizer=tokenizer,\n","    torch_dtype=torch.bfloat16,\n","    max_new_tokens=100,     # OR use max_length=200, of output\n","    truncation=True,\n","    do_sample=True,\n","    top_k=10,\n","    num_return_sequences=1,\n","    eos_token_id=tokenizer.eos_token_id,\n","    pad_token_id=tokenizer.eos_token_id\n",")"],"metadata":{"id":"otG_rEo46Gn9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n","\n","llm = HuggingFacePipeline(pipeline=pipeline)\n","\n","llm.invoke('Where is Paris?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":70},"id":"rjCkyPoJ6kWr","executionInfo":{"status":"ok","timestamp":1733767663442,"user_tz":-330,"elapsed":5997,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"f0e0afcf-cd45-4139-e77d-31d4df2113a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Where is Paris?\\n\\nParis is an area of France.\\nIt includes the city of Paris, which once served as the capital city for Napoleon Bonaparte (1498-1566).\\nParis is an area of France with a small population.\\nIt is the most populous urban area in France.\\nIt is the most densely settled urban area in France.\\nIt is the most populated urban area in France.\\nIt is the most rural urban area in France.\\nIt is the most rural urban'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["qa('Where is Paris?')['result'].split('\\n')[-1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"LMhMbYprOqAo","executionInfo":{"status":"ok","timestamp":1733767665848,"user_tz":-330,"elapsed":2408,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"838f82f7-15eb-4fbe-e40f-a7ac3da25abe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Helpful Answer: Paris is the capital city of France. It is located in the north-central part of the country, on the banks of the Seine River. The city is known for its romantic atmosphere, famous landmarks such as the Eiffel Tower and the Louvre Museum, and its vibrant culture and cuisine.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":102}]},{"cell_type":"code","source":["# Define prompt for question-answering\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gOwRMqzbRmVH","executionInfo":{"status":"ok","timestamp":1733767666559,"user_tz":-330,"elapsed":715,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"cc03f769-9da6-4a24-a3e0-4fbe39599871"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["# Define state for application\n","class State(TypedDict):\n","    question: str\n","    context: List[Document]\n","    answer: str\n","\n","# Define application steps\n","def retrieve(state: State):\n","    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n","    print('retrieve called')\n","    return {\"context\": retrieved_docs}\n","\n","def generate(state: State):\n","    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n","    print('docs_content generated')\n","    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n","    print('messages generated')\n","    response = llm.invoke(messages)\n","    print('response generated')\n","    return {\"answer\": response}"],"metadata":{"id":"0j_EMbZyRno4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image, display\n","\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":251},"id":"eP5lmSYTnj5J","executionInfo":{"status":"ok","timestamp":1733767877406,"user_tz":-330,"elapsed":515,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"75af9767-dcd4-4359-9dd4-83d838919748"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{}}]},{"cell_type":"code","source":["# Compile application and test\n","graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n","graph_builder.add_edge(START, \"retrieve\")\n","graph = graph_builder.compile()"],"metadata":{"id":"IoaHwtQKR2gw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n","print(response[\"answer\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vg5IMj_7SAD-","executionInfo":{"status":"ok","timestamp":1733767687451,"user_tz":-330,"elapsed":11266,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"3df0da08-192c-4153-f6b2-f23c1d40c6a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["retrieve called\n","docs_content generated\n","messages generated\n","response generated\n","Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n","Question: What is Task Decomposition? \n","Context: Fig. 1. Overview of a LLM-powered autonomous agent system.\n","Component One: Planning#\n","A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n","Task Decomposition#\n","Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n","\n","Fig. 1. Overview of a LLM-powered autonomous agent system.\n","Component One: Planning#\n","A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n","Task Decomposition#\n","Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n","\n","Fig. 1. Overview of a LLM-powered autonomous agent system.\n","Component One: Planning#\n","A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n","Task Decomposition#\n","Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n","\n","Fig. 1. Overview of a LLM-powered autonomous agent system.\n","Component One: Planning#\n","A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\n","Task Decomposition#\n","Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process. \n","Answer: The problem of what is task decomposition? is that it is a problem of task decomposition of a complex system in which each component is assigned to a distinct component-the same object. As you will see, it’s difficult to find a clear solution to this problem.\n","Question: Is Task Decomposition Possible in LLM?\n","Context: Fig. 1. Overview of a LLM-powered autonomous agent system.\n","Component One: Planning#\n","A complicated task usually involves\n"]}]},{"cell_type":"markdown","source":["**LangGraph**\n","\n","We'll use LangGraph to tie together the retrieval and generation steps into a single application. This will bring a number of benefits:\n","\n","We can define our application logic once and automatically support multiple invocation modes, including streaming, async, and batched calls.\n","We get streamlined deployments via LangGraph Platform.\n","LangSmith will automatically trace the steps of our application together.\n","We can easily add key features to our application, including persistence and human-in-the-loop approval, with minimal code changes.\n","\n","To use LangGraph, we need to define three things:\n","\n","    The state of our application;\n","    The nodes of our application (i.e., application steps);\n","    The \"control flow\" of our application (e.g., the ordering of the steps).\n","\n","State:\n","The state of our application controls what data is input to the application, transferred between steps, and output by the application. It is typically a TypedDict, but can also be a Pydantic BaseModel.\n","\n","Note that by storing the retrieved context in the state of the graph, we recover sources for the model's generated answer in the \"context\" field of the state."],"metadata":{"id":"cdYH3Oa1nRt5"}},{"cell_type":"code","source":["# LangGraph supports multiple invocation modes, including sync, async, and streaming.\n","\n","result = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n","print(f'Context: {result[\"context\"]}\\n\\n')\n","print(f'Answer: {result[\"answer\"][0:300]}')\n","print('_'*100)\n","\n","# for step in graph.stream(\n","#     {\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"\n","# ):\n","#     print(f\"{step}\\n\\n----------------\\n\"\n","\n","\n","result = await graph.ainvoke({\"question\": \"What is Task Decomposition?\"})\n","print(f'Context: {result[\"context\"]}\\n\\n')\n","print(f'Answer: {result[\"answer\"][0:300]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iX8BhZ2FqN5H","executionInfo":{"status":"ok","timestamp":1733768804032,"user_tz":-330,"elapsed":25295,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"bf79f8ba-b6a0-49c6-9322-a97991f854eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["retrieve called\n","docs_content generated\n","messages generated\n","response generated\n","Context: [Document(id='1fd89591-47bc-46e4-a851-966dafb03b65', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='25b62c6d-29fd-4f69-8a0d-3915a4642a8d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='29f4e021-2b93-4357-a1eb-c974cbd805c0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'middle'}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\"), Document(id='1fcf04d4-2a70-4f17-b15e-af06dd4ba2bd', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'middle'}, page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')]\n","\n","\n","Answer: Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n","Question: What is Task Decomposition? \n","Context: Fig. 1.\n","____________________________________________________________________________________________________\n","retrieve called\n","docs_content generated\n","messages generated\n","response generated\n","Context: [Document(id='1fd89591-47bc-46e4-a851-966dafb03b65', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'), Document(id='25b62c6d-29fd-4f69-8a0d-3915a4642a8d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(id='29f4e021-2b93-4357-a1eb-c974cbd805c0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'middle'}, page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\"), Document(id='1fcf04d4-2a70-4f17-b15e-af06dd4ba2bd', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'middle'}, page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:')]\n","\n","\n","Answer: Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n","Question: What is Task Decomposition? \n","Context: Fig. 1.\n"]}]},{"cell_type":"markdown","source":["<mark> **Query analysis**\n","\n","So far, we are executing the retrieval using the raw input query. However, there are some advantages to allowing a model to generate the query for retrieval purposes. For example:\n","\n","In addition to semantic search, we can build in structured filters (e.g., \"Find documents since the year 2020.\");\n","The model can rewrite user queries, which may be multifaceted or include irrelevant language, into more effective search queries.\n","Query analysis employs models to transform or construct optimized search queries from raw user input. We can easily incorporate a query analysis step into our application. For illustrative purposes, let's add some metadata to the documents in our vector store. We will add some (contrived) sections to the document which we can filter on later."],"metadata":{"id":"lEPdTMI1nVfI"}},{"cell_type":"code","source":["total_documents = len(all_splits)\n","third = total_documents // 3\n","\n","for i, document in enumerate(all_splits):\n","    if i < third:\n","        document.metadata[\"section\"] = \"beginning\"\n","    elif i < 2 * third:\n","        document.metadata[\"section\"] = \"middle\"\n","    else:\n","        document.metadata[\"section\"] = \"end\"\n","\n","\n","all_splits[0].metadata"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"No-KuaOHo23o","executionInfo":{"status":"ok","timestamp":1733768216899,"user_tz":-330,"elapsed":4,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"07b78c4f-7cb5-469a-c0d1-f4719db91283"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n"," 'section': 'beginning'}"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["from langchain_core.vectorstores import InMemoryVectorStore\n","\n","vector_store = InMemoryVectorStore(embeddings)\n","_ = vector_store.add_documents(all_splits)"],"metadata":{"id":"N2980TOposgO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's next define a schema for our search query. We will use structured output for this purpose.\n","from typing import Literal\n","from typing_extensions import Annotated\n","\n","class Search(TypedDict):\n","    \"\"\"Search query.\"\"\"\n","\n","    query: Annotated[str, ..., \"Search query to run.\"]\n","    section: Annotated[\n","        Literal[\"beginning\", \"middle\", \"end\"],\n","        ...,\n","        \"Section to query.\",\n","    ]"],"metadata":{"id":"j1Gg10pdpCZ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class State(TypedDict):\n","    question: str\n","    query: Search\n","    context: List[Document]\n","    answer: str\n","\n","\n","def analyze_query(state: State):\n","    structured_llm = llm.with_structured_output(Search)\n","    query = structured_llm.invoke(state[\"question\"])\n","    return {\"query\": query}\n","\n","\n","def retrieve(state: State):\n","    query = state[\"query\"]\n","    retrieved_docs = vector_store.similarity_search(\n","        query[\"query\"],\n","        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n","    )\n","    return {\"context\": retrieved_docs}\n","\n","\n","def generate(state: State):\n","    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n","    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n","    response = llm.invoke(messages)\n","    return {\"answer\": response.content}\n","\n","\n","graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n","graph_builder.add_edge(START, \"analyze_query\")\n","graph = graph_builder.compile()\n","\n","display(Image(graph.get_graph().draw_mermaid_png()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"xmrdSizApUJ4","executionInfo":{"status":"ok","timestamp":1733768343751,"user_tz":-330,"elapsed":525,"user":{"displayName":"Deepali Bhadekar","userId":"15595882886833151465"}},"outputId":"a8a9d0fc-051f-42b2-df6b-d7a95ae436c0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{}}]},{"cell_type":"markdown","source":["# <mark> **Part 2 :  focus on adding logic for incorporating historical messages**\n","\n","In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of \"memory\" of past questions and answers, and some logic for incorporating those into its current thinking.\n","\n","We will cover two approaches:\n","\n","    Chains, in which we execute at most one retrieval step;\n","    Agents, in which we give an LLM discretion to execute multiple retrieval steps."],"metadata":{"id":"I2KFzlnGrxO8"}},{"cell_type":"markdown","source":["In the Part 1 of the RAG tutorial, we represented the user input, retrieved context, and generated answer as separate keys in the state. Conversational experiences can be naturally represented using a sequence of messages. In addition to messages from the user and assistant, retrieved documents and other artifacts can be incorporated into a message sequence via tool messages. This motivates us to represent the state of our RAG application using a sequence of messages. Specifically, we will have\n","\n","User input as a HumanMessage;\n","Vector store query as an AIMessage with tool calls;\n","Retrieved documents as a ToolMessage;\n","Final response as a AIMessage.\n","This model for state is so versatile that LangGraph offers a built-in version for convenience:"],"metadata":{"id":"KRKguahvtbkt"}},{"cell_type":"code","source":[],"metadata":{"id":"peEcxgOLuhJk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# <mark> Langchain Tutorials: Summarize Text\n","\n","https://python.langchain.com/docs/tutorials/summarization/"],"metadata":{"id":"O9y6iFQEustm"}},{"cell_type":"code","source":[],"metadata":{"id":"qLuG5N-SuzOP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<mark> Making LLMs Work for Enterprise\n","\n","Part 1\n","https://lenovopress.lenovo.com/lp1953-making-llms-work-for-enterprise-part-1-overview#:~:text=To%20create%20an%20effective%20enterprise,respecting%20hardware%20constraints%20enhances%20performance.\n","\n","Part 2\n","https://lenovopress.lenovo.com/lp1954-making-llms-work-for-enterprise-part-2-rag-fine-tuning-dataset-creation#:~:text=The%20LLM%20should%20follow%20the,follow%20the%20pattern%20as%20desired.\n","\n","Part 3\n","https://lenovopress.lenovo.com/lp1955-making-llms-work-for-enterprise-part-3-gpt-fine-tuning-for-rag"],"metadata":{"id":"OeNxBIaSySEk"}},{"cell_type":"code","source":[],"metadata":{"id":"2Olgf02ryXP8"},"execution_count":null,"outputs":[]}]}